# 어제 복습
# I/O = Input & Output
# RAG, Agent

# Agent를 쓰지 않았을 때는 모른다고 대답했던 답변에서 wikipedia를 에이전트로 건네줬더니 AI 스스로 위키피디아를 이용해 해당 검색 결과를 불러옴

# Retrieval
# 1. Indexing Phase
# document생성
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader('./snow-white.pdf')
documents = loader.load()

for doc in documents:
    doc.page_content = doc.page_content.replace('\n', ' ')
    # doc.page_content만 했을 때의 예시
    # \n왕비는 진실만을 말하는 요술 거울에게 늘 이렇게 물\n었어요.\n“거울아, 거울아. 이 세상에서 누가 가장 아름답니?”\n 
    # \n 개행문자를 공백 혹은 빈 문자열로 replace
documents    

# Text Splitter
# RecursiveCharacterTextSplitter : 긴 텍스트를 재귀적으로 분석하여 작은 조각으로 분할하는 TextSplitter의 구현체

from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    # 글자 수 몇 개를 기준으로 해서 쪼갤거냐(보통은 1000을 기준으로 함)
    chunk_size=100,
    # 문단 단위로 쪼개온다고 할 때, 유실이 될 수도 있음. 그래서 문단 앞 뒤를 살짝 겹치게 쪼개와야한다.
    # chunk_overlap=겹칠 글자 수
    chunk_overlap=20
)
docs = splitter.split_documents(documents)
print(len(docs))  # 26
docs



# 오늘 수업
# 임베딩 모델(어제 한 결과물 벡터화)
1. 임베딩 모델 사용 위해 필요 요소 import
from langchain_openai.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    # vector store가 알아서 임베딩 모델을 저장해줌
)

2. 벡터 스토어 사용 위해 필요 요소 install
%pip install langchain-chroma

from langchain_chroma.vectorstores import Chroma

3. Retrival 기반, 유사도 검색
from langchain_chroma.vectorstores import Chroma

vector_store = Chroma.from_documents(docs, embeddings)

query = '백설공주와 왕비 중에 누가 더 아름다운가요?'

# retrieval 사용 않고 벡터 스토어에 직접 조회하는 방법도 있다.
# similarity_search 사용
retrievals = vector_store.similarity_search_with_score(query)
retrievals

"""
[(Document(id='82f069db-d2ed-4bd5-80d4-1a2896cb70a3', metadata={'title': 'PowerPoint 프레젠테이션', 'page': 2, 'creationdate': '2023-09-12T11:20:24+09:00', 'author': 'PC', 'creator': 'Microsoft® PowerPoint® 2013', 'producer': 'Microsoft® PowerPoint® 2013', 'total_pages': 6, 'page_label': '3', 'source': './snow-white.pdf', 'moddate': '2023-09-12T11:20:24+09:00'}, page_content='왕비는 다시 요술 거울에게 누가 가장 아름다운 지 물었어요. “왕비님도 아름답지만 백설공주님이 천배는 더 아름답습니다.” “사냥꾼이 날 속였구나. 내가 직접 해치우겠어!”'),
  0.8201914429664612),
 (Document(id='d4d6b554-966b-47d4-9b79-351ec27c9054', metadata={'title': 'PowerPoint 프레젠테이션', 'producer': 'Microsoft® PowerPoint® 2013', 'total_pages': 6, 'moddate': '2023-09-12T11:20:24+09:00', 'source': './snow-white.pdf', 'creator': 'Microsoft® PowerPoint® 2013', 'creationdate': '2023-09-12T11:20:24+09:00', 'page_label': '2', 'page': 1, 'author': 'PC'}, page_content='거울아. 이 세상에서 누가 가장 아름답니?” “왕비님도 아름답지만 백설공주가 더 아름답습니다.” 화가 난 왕비는 사냥꾼을 불렀어요. 왕비는 사냥꾼에게 백설공주를 죽이라고'),
  0.8282550573348999),
 (Document(id='629c8c18-4130-4dd0-bab2-ea99ca174668', metadata={'total_pages': 6, 'author': 'PC', 'source': './snow-white.pdf', 'creator': 'Microsoft® PowerPoint® 2013', 'producer': 'Microsoft® PowerPoint® 2013', 'moddate': '2023-09-12T11:20:24+09:00', 'page_label': '5', 'creationdate': '2023-09-12T11:20:24+09:00', 'page': 4, 'title': 'PowerPoint 프레젠테이션'}, page_content='“백설공주님, 못된 왕비의 꾐에 넘어갔군요.” “여전히 아름다운 우리 공주님을 캄캄한 땅속에 묻을 순 없어.” “오래오래 볼 수 있게 유리 관에 모시자.” 어느 날, 한 왕자가'),
  0.9228578209877014),
 (Document(id='980af31e-cd3a-41b8-b483-7c334da090c8', metadata={'producer': 'Microsoft® PowerPoint® 2013', 'moddate': '2023-09-12T11:20:24+09:00', 'title': 'PowerPoint 프레젠테이션', 'total_pages': 6, 'source': './snow-white.pdf', 'page_label': '2', 'creationdate': '2023-09-12T11:20:24+09:00', 'creator': 'Microsoft® PowerPoint® 2013', 'author': 'PC', 'page': 1}, page_content='왕은 아름다운 새 왕비를 맞았어요. 그런데 새 왕비는 자기보다 아름다운 사람을 두고 보 지 못했어요. 왕비는 진실만을 말하는 요술 거울에게 늘 이렇게 물 었어요. “거울아,'),
  1.043031930923462)]
"""

# Retrieval 사용 검색
retrieval = vector_store.as_retriever(
    search_tyep='similarity',
    search_kwargs={'k':3}  # kwargs = 몇 건의 결과를 원하는지
)
# vector로 검색하는 것은 키워드 기반이 아니라 '의미'기반이다.

retrievals = retrieval.batch([query]) # ([]) 리스트 안에 query 전달
retrievals

"""
[[Document(id='82f069db-d2ed-4bd5-80d4-1a2896cb70a3', metadata={'source': './snow-white.pdf', 'page_label': '3', 'author': 'PC', 'page': 2, 'creationdate': '2023-09-12T11:20:24+09:00', 'creator': 'Microsoft® PowerPoint® 2013', 'producer': 'Microsoft® PowerPoint® 2013', 'title': 'PowerPoint 프레젠테이션', 'moddate': '2023-09-12T11:20:24+09:00', 'total_pages': 6}, page_content='왕비는 다시 요술 거울에게 누가 가장 아름다운 지 물었어요. “왕비님도 아름답지만 백설공주님이 천배는 더 아름답습니다.” “사냥꾼이 날 속였구나. 내가 직접 해치우겠어!”'),
  Document(id='d4d6b554-966b-47d4-9b79-351ec27c9054', metadata={'page': 1, 'creationdate': '2023-09-12T11:20:24+09:00', 'source': './snow-white.pdf', 'page_label': '2', 'creator': 'Microsoft® PowerPoint® 2013', 'author': 'PC', 'total_pages': 6, 'title': 'PowerPoint 프레젠테이션', 'producer': 'Microsoft® PowerPoint® 2013', 'moddate': '2023-09-12T11:20:24+09:00'}, page_content='거울아. 이 세상에서 누가 가장 아름답니?” “왕비님도 아름답지만 백설공주가 더 아름답습니다.” 화가 난 왕비는 사냥꾼을 불렀어요. 왕비는 사냥꾼에게 백설공주를 죽이라고'),
  Document(id='629c8c18-4130-4dd0-bab2-ea99ca174668', metadata={'page': 4, 'creationdate': '2023-09-12T11:20:24+09:00', 'source': './snow-white.pdf', 'producer': 'Microsoft® PowerPoint® 2013', 'total_pages': 6, 'title': 'PowerPoint 프레젠테이션', 'moddate': '2023-09-12T11:20:24+09:00', 'author': 'PC', 'page_label': '5', 'creator': 'Microsoft® PowerPoint® 2013'}, page_content='“백설공주님, 못된 왕비의 꾐에 넘어갔군요.” “여전히 아름다운 우리 공주님을 캄캄한 땅속에 묻을 순 없어.” “오래오래 볼 수 있게 유리 관에 모시자.” 어느 날, 한 왕자가')]]
"""

## 2. Retrieval and Generation Phase

### 프롬프트 생성
1. 사용자 질의
2. 검색된 문서 도출

# 프롬프트 생성
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate([
    ('system', 'You are kindergarten teacher who promotes dream and hope to the kids. Answer based on context while response as much as positive when the kid asked.'),
    ('user', '''
     사용자의 질문에 context만을 이용해 답변해주세요.
     
     question: {query}

     context: {context}
    ''' )
])

prompt.invoke({'query': query, 'context':retrieval})

# system에 넣어준 페르소나가 system message로 잘 들어간 것을 확인할 수 있다.
# ChatPromptValue(messages=[SystemMessage(content='You are kindergarten teacher who promotes dream and hope to the kids. Answer based on context while response as much as positive when the kid asked.', additional_kwargs={}, response_metadata={}), HumanMessage(content="\n     사용자의 질문에 context만을 이용해 답변해주세요.\n\n     question: 백설공주와 왕비 중에 누가 더 아름다운가요?\n\n     context: tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x0000020ECE63FCB0> search_kwargs={'k': 3}\n    ", additional_kwargs={}, response_metadata={})])

# 모델 생성
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model_name='gpt-4.1',
    temperature=0.5
)

# 체인 생성
# outputparser 이용
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
# RunnablePassthrough: chain 생성 후 invoke 할 때 그대로 출력할 수 있게 만든다
# (수업 노트) invoke()에 전달된 인자를 받아서 그대로 출력해주는 역할

chain={'query': RunnablePassthrough(), 'context': retrieval} | prompt | model | StrOutputParser()
# 만든 쿼리와 context를 prompt에 전달 -> model(gpt-4.1) 전달 -> StrOutputParser() 전달('String으로 결과를 받아보겠다'라는 의미)

chain.invoke(query)

"""
'질문에 대해 이야기해 줄게요! 이야기 속에서 요술 거울이 “왕비님도 아름답지만 백설공주가 더 아름답습니다.”라고 대답했어요. 또, “백설공주님이 천배는 더 아름답습니다.”라고도 했지요. 그래서 이야기에서는 백설공주가 더 아름답다고 나와 있답니다. 하지만 우리 모두는 각자만의 아름다움이 있어요! 너도, 나도, 백설공주도, 왕비도 모두 특별하고 소중하단다. 아름다움은 겉모습뿐만 아니라 마음에서도 나온다는 걸 꼭 기억해 줘!'
"""

# pdf 이야기 내용 바탕으로 물어보고 싶은 질문
chain.invoke('백설공주는 어디로 도망쳤나요?')
"""  '백설공주는 울면서 숲으로 도망쳤어요. 숲에는 새로운 친구들과 신나는 모험이 기다리고 있었답니다!' """

# 다른 예시
chain = prompt | model | StrOutputParser()

chain.invoke({'query': '난쟁이는 몇명인가요?', 'context': retrieval})
""" '난쟁이는 일곱 명이에요! 동화 속에서 난쟁이들은 언제나 힘을 합쳐서 멋진 모험을 떠나지요. 너도 언젠가 멋진 친구들과 함께 신나는 모험을 할 수 있을 거예요!'  """


# 요리에 어울리는 와인 추천받기

# 필요한 것들 import
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate([
    # 리스트 안 쪽에 튜플 형식으로 넣어줌
    ('system', '''
    Persona: You are a knowledgeable and experienced sommelier with a passion for wine and food pairings. You possess an extensive understanding of various wine regions, grape varieties, and tasting notes. Your demeanor is friendly and approachable, making wine accessible to everyone, whether they are novices or connoisseurs.

    Role: As a sommelier, your role is to provide expert recommendations for wine selections that perfectly complement a variety of cuisines. You guide individuals in exploring new wines and help them understand the intricacies of wine tasting. Your goal is to enhance their dining experience by matching the right wine with the right dish.

    Examples:

    When asked for a wine recommendation for grilled garlic butter shrimp, you suggest a Chardonnay or Albariño, explaining how the wine's acidity balances the richness of the dish.
    If someone inquires about affordable yet high-quality wines, you recommend specific options from different regions, highlighting their flavor profiles and food pairings.
    When discussing wine storage, you provide practical advice on temperature, humidity, and ideal conditions to preserve wine quality.
     '''),
     ('human', '''
    다음 요리에 어울리는 와인을 한국어로 답변해주세요.
    요리명: {query}
    ''')
])

model = ChatOpenAI(
    model_name='gpt-4.1',
    temperature=1
)

output_parser = StrOutputParser()

chain = prompt | model | output_parser

response = chain.invoke({'query': '떡갈비'})
print(response)


# 와인에 어울리는 요리 추천받기
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 함수 정의
def recommend_dishes(query):
    prompt= ChatPromptTemplate([
    ('system', """
    Persona: You are a knowledgeable and experienced chef who has been work as sommlier too. And you have a passion for wine and food pairings. You possess an extensive understanding of various wine regions, grape varieties, and tasting notes. Your demeanor is friendly and approachable, making wine accessible to everyone, whether they are novices or connoisseurs.

    Role: As a chef, your role is to provide expert recommendations for food selections that perfectly complement a variety of wines. You guide individuals in exploring new foods and help them understand the intricacies of wine tasting. Your goal is to enhance their dining experience by matching the right dish with the right wine.

    Examples:

    When asked for a food recommendation for pinot noir, you suggest a gambas de alahiyo, explaining how the food's tastes balances with the freshness of the wine.
    If someone inquires about affordable yet high-quality wines, you recommend specific options from different regions, highlighting their flavor profiles and food pairings.
    """),
    HumanMessagePromptTemplate.from_template([
        {'text': '다음 와인에 어울리는 요리를 한국말로 답변해주세요.'},
        {'image_url': '{image_url}'}
        ])
    ])

    model = ChatOpenAI(
        model_name='gpt-4.1',
        temperature=1
    )

    output_parser= StrOutputParser()

    chain = prompt | model | output_parser

    return chain.invoke(query)

print(recommend_dishes({'image_url': 'https://images.vivino.com/thumbs/iE_y2NRLSWKWw--znVRE3Q_pb_x600.png'}))

chain = prompt | model | output_parser

"""
이 와인은 독일 라인가우 지역의 **리슬링 트로켄(Riesling Trocken)** 와인입니다. 리슬링은 산미가 높고, 깔끔하면서도 과일향이 풍부한 것이 특징입니다. 특히 드라이(트로켄)한 스타일은 음식과의 페어링이 매우 좋아요.

### 어울리는 한식 요리 추천

1. **해산물 파전**
   - 리슬링 특유의 산미와 가벼움이 해산물과 잘 어울립니다. 특히 파전의 기름짐을 산뜻하게 잡아줍니다.

2. **양념치킨**
   - 은은한 단맛과 함께 산도가 높아 매콤달콤한 양념치킨과 궁합이 좋습니다.

3. **골뱅이무침**
   - 매콤하고 새콤한 골뱅이무침의 소스와 리슬링의 산미가 조화를 이룹니다.

4. **산낙지 초무침**
   - 신선한 산낙지와 매콤, 새콤한 양념이 리슬링의 청량감과 찰떡입니다.

5. **오이무침, 나박김치 등 아삭하고 산뜻한 반찬**
   - 산도를 강조하는 가벼운 반찬들과도 조화롭습니다.

### 페어링 팁
리슬링 트로켄은 조미료가 강하지 않고, 재료 본연의 맛을 살리는 음식, 특히 신맛과 매운맛이 있는 요리에 잘 어울립니다. 생선회나 해산물 요리도 좋고, 새콤달콤한 한식과도 멋진 궁합을 자랑해요!

혹시 더 자세한 음식이나 다른 장르(예: 양식, 일식 등)와의 페어링이 궁금하다면 추가로 질문해 주세요!
"""

# 요리 이미지로 해당 요리에 어울리는 와인 추천받기
# 필요한 것들 import
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate([
    # 리스트 안 쪽에 튜플 형식으로 넣어줌
    ('system', '''
    Persona: You are a knowledgeable and experienced sommelier with a passion for wine and food pairings. You possess an extensive understanding of various wine regions, grape varieties, and tasting notes. Your demeanor is friendly and approachable, making wine accessible to everyone, whether they are novices or connoisseurs.

    Role: As a sommelier, your role is to provide expert recommendations for wine selections that perfectly complement a variety of cuisines. You guide individuals in exploring new wines and help them understand the intricacies of wine tasting. Your goal is to enhance their dining experience by matching the right wine with the right dish.

    Examples:

    When asked for a wine recommendation for grilled garlic butter shrimp, you suggest a Chardonnay or Albariño, explaining how the wine's acidity balances the richness of the dish.
    If someone inquires about affordable yet high-quality wines, you recommend specific options from different regions, highlighting their flavor profiles and food pairings.
    When discussing wine storage, you provide practical advice on temperature, humidity, and ideal conditions to preserve wine quality.
     '''),
     ('human', '''
    다음 요리에 어울리는 와인을 한국어로 답변해주세요.
    요리명: {img_url}
    ''')
])

model = ChatOpenAI(
    model_name='gpt-4.1',
    temperature=1
)

output_parser = StrOutputParser()

chain = prompt | model | output_parser

response = chain.invoke({'img_url': 'https://static.wtable.co.kr/image/production/service/recipe/1944/19517cdf-75c9-4e9d-87cf-4eb682808d05.jpg?size=800x800'})
print(response)

"""
사진 속 요리는 닭볶음탕(매콤한 닭고기 조림)으로 보입니다. 매콤하고 진한 양념의 한국식 닭볶음탕에는 다음과 같은 와인을 추천드립니다.

**추천 와인 1: 쉬라(Shiraz/Syrah)**
- 매콤한 양념에는 적당한 바디감과 풍부한 과일 향을 가진 쉬라가 잘 어울립니다. 쉬라의 스파이시한 노트와 다크 베리 향이 닭볶음탕의 매콤함과 잘 어우러집니다.

**추천 와인 2: 젠핀델(Zinfandel)**
- 진한 풍미와 살짝 달큰한 면이 있는 젠핀델도 닭볶음탕의 매콤달콤한 양념과 조화를 이룹니다. 특히 캘리포니아 젠핀델을 추천드립니다.

**추천 와인 3: 램브루스코(Lambrusco)**
- 가볍게 마실 수 있는 약간 스파클링한 레드 와인으로, 매운 음식과 잘 어울립니다. 적당한 단맛과 산미가 매운맛을 부드럽게 해주어 부담 없이 즐길 수 있습니다.

**와인 페어링 팁:**
와인을 너무 차갑게 혹은 너무 따뜻하게 마시지 말고, 레드 와인은 약 16~18℃ 정도로 살짝 식혀 드시면 음식과 더욱 잘 어울립니다.

더 궁금한 점이 있으면 언제든 말씀해 주세요!
"""
# 갈비찜인데 닭볶음탕으로 인식했다....... 어떻게 그럴 수가! 저렇게 촉촉하고 부드러워 보이는 갈비찜을 다른 걸로 착각하다니



# Kaggle : 가장 큰 공개 데이터 회사
# Pinecone Vector DB
1. Environment settings
from dotenv import load_dotenv
load_dotenv()

2. load csv file that downloaded from kaggle
# csv file load
from langchain_community.document_loaders import CSVLoader

loader = CSVLoader('./winemag-data-130k-v2.csv', encoding='utf-8')
# csv 파일 깨지지 않게 만들기 위해 encoding='utf-8' 추가

documents = loader.load()

print(len(documents)) # 129971
documents[0]
# Document(metadata={'source': './winemag-data-130k-v2.csv', 'row': 0}, page_content=": 0\ncountry: Italy\ndescription: Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\ndesignation: Vulkà Bianco\npoints: 87\nprice: \nprovince: Sicily & Sardinia\nregion_1: Etna\nregion_2: \ntaster_name: Kerin O’Keefe\ntaster_twitter_handle: @kerinokeefe\ntitle: Nicosia 2013 Vulkà Bianco  (Etna)\nvariety: White Blend\nwinery: Nicosia")

# pinecone 사용
# %pip install langchain_pinecone

from pinecone import Pinecone, ServerlessSpec
pc = Pinecone(api_key="{my_secret_api_key)")

# .env 파일에 pinecone API key 저장
# pinecone에서 create new index하고 "ai-sommelier-rag"라는 이름으로 인덱스 생성, configuration은 text-embedding-3-small

# Generate Pinecone Vector DB client 
# Pinecone 클라이언트
from langchain_pinecone import PineconeVectorStore
import os

PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME')
PINECONE_NAMESPACE = os.getenv('PINECONE_NAMESPACE')
PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')

vector_db = PineconeVectorStore.from_documents(
    documents,
    embeddings,
    # 아래 3가지는 env파일에서 값 불러다가 사용(import os 필수)
    index = PINECONE_INDEX_NAME,
    namespace=PINECONE_NAMESPACE,
    pinecone_api_key=PINECONE_API_KEY
)

# AI Sommelier RAG
# 1. 환경 설정
from dotenv import load_dotenv
load_dotenv()

from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

#아까 프롬프트 전달하고 했던 것을 이번에는 함수로 전달
def describe_dish_flavor(query):
    prompt = ChatPromptTemplate.from_messages([
        ('system', '''
         Persona: You are professional gourmet who works at this field over the 10 years. You have a deep understanding of culinary techniques, flavor profiles, and ingredient pairings. You have a passion for exploring diverse cuisines and an ability to articulate the sensory experience of food. Your insights are backed by both practical experience and theoretical knowledge, making you a trusted source in the culinary field.
        
         Role: As a professional gourmet, your role is to analyze the flavors, textures, and aromas of various dishes. You provide detailed evaluations of ingredients and cooking methods, helping others understand how to create balanced and harmonious dishes. You also educate individuals on how to enhance their cooking skills and appreciate the art of gastronomy.
         
         Examples:

         When asked to analyze the flavor profile of a dish, you describe the balance between acidity, sweetness, bitterness, and umami, explaining how these elements interact to create a complex taste experience.
         If someone inquires about the best techniques for enhancing a specific ingredient, you offer practical advice, such as how to caramelize onions for depth of flavor or how to properly season meat to bring out its natural taste.
         When discussing food pairings, you suggest complementary ingredients and flavors, explaining the rationale behind each choice, such as pairing citrus with seafood to brighten the dish or using herbs to elevate the overall taste.
         '''),
         ('user', '이미지를 분석해서 요리명과 예상되는 풍미를 한 문장으로 요약해줘.')
    ])
    prompt += HumanMessagePromptTemplate.from_template([query])
    model= ChatOpenAI(model_name='gpt-4.1', temperature=0)
    output_parser = StrOutputParser()

    # chain 연결
    chain = prompt | model | output_parser
    return chain


# runnable lambda 사용
# RunnableLamba란? : wrapper class. 위에서 일반 함수로 선언한 describe_dish_flavor를 RL로 감싸주면 Langchain 객체가(Langchain객체인 것처럼) 된다.
# (일반 함수(예: describe_dish_flavor)를 LangChain 표현식 언어(LCEL)의 일부인 Runnable 객체로 변환해주는 래퍼 클래스)
# 그러면 Langchain에서 사용 가능(이를 통해 일반 함수도 LangChain 체인의 일부로 파이프 연결a|b 하거나 병렬 실행할 수 있게 됩니다.)
from langchain_core.runnables import RunnableLambda

chain = RunnableLambda(describe_dish_flavor)
chain.invoke({'image_url': 'https://www.recipetineats.com/tachyon/2023/06/Hungarian-Goulash-_6.jpg?resize=900%2C1125&zoom=0.72'})
"""
'이 요리는 ‘비프 스튜’로, 부드럽게 익은 소고기와 감자, 당근이 진한 토마토 베이스의 국물과 어우러져 깊고 풍부한 감칠맛과 은은한 허브 향이 조화를 이루는 따뜻한 풍미가 예상됩니다.'
"""

# Pinecone에 넣었던 records의 리뷰에서 와인 검색
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
import os

PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME')
PINECONE_NAMESPACE = os.getenv('PINECONE_NAMESPACE')
PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')

# 와인 찾기 위한 함수 정의
def search_wines(query):
    # 이 함수에서 해줘야할 것 """ """ 안에 있음
    """
    :param query: 요리설명 텍스트
    :return: 요리 설명에 어울리는 와인 추천 결과
    """

    # 임베딩 모델 만들기
    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')

    # vector db를 쓸 거예요.
    vector_db = PineconeVectorStore(
        embeddings=embeddings,
        index_name=PINECONE_INDEX_NAME,
        namespace=PINECONE_NAMESPACE,
        pinecone_api_key=PINECONE_API_KEY
    )
    results = vector_db.similarity_search(
        query,
        k=5,
        namespace=PINECONE_NAMESPACE
    )
    return {
        "dish_flavor": query,
        "wine_reviews": "\n".join([doc.page_content for doc in results])
    }

# chain을 runnablelambda로 감싸주는 과정 필요
chain = RunnableLambda(search_wines)
chain.invoke("이 요리는 ‘비프 스튜’로, 부드럽게 익은 소고기와 감자, 당근이 진한 토마토 베이스의 국물과 어우러져 깊고 풍부한 감칠맛과 은은한 허브 향이 조화를 이루는 따뜻한 풍미가 예상됩니다.")

# TROUBLE OCCURRED
TypeError                                 Traceback (most recent call last)
Cell In[8], line 3
      1 # chain을 runnablelambda로 감싸주는 과정 필요
      2 chain = RunnableLambda(search_wines)
----> 3 chain.invoke("이 요리는 ‘비프 스튜’로, 부드럽게 익은 소고기와 감자, 당근이 진한 토마토 베이스의 국물과 어우러져 깊고 풍부한 감칠맛과 은은한 허브 향이 조화를 이루는 따뜻한 풍미가 예상됩니다.")

File c:\Users\User\miniconda3\envs\pythonstudy_env\Lib\site-packages\langchain_core\runnables\base.py:4775, in RunnableLambda.invoke(self, input, config, **kwargs)
   4761 """Invoke this Runnable synchronously.
   4762 
   4763 Args:
   (...)   4772     TypeError: If the Runnable is a coroutine function.
   4773 """
   4774 if hasattr(self, "func"):
-> 4775     return self._call_with_config(
   4776         self._invoke,
   4777         input,
   4778         ensure_config(config),
   4779         **kwargs,
   4780     )
   4781 msg = "Cannot invoke a coroutine function synchronously.Use `ainvoke` instead."
   4782 raise TypeError(msg)

File c:\Users\User\miniconda3\envs\pythonstudy_env\Lib\site-packages\langchain_core\runnables\base.py:1939, in Runnable._call_with_config(self, func, input_, config, run_type, serialized, **kwargs)
   1935     child_config = patch_config(config, callbacks=run_manager.get_child())
...
     33     "dish_flavor": query,
     34     "wine_reviews": "\n".join([doc.page_content for doc in results])
     35 }

TypeError: PineconeVectorStore.__init__() got an unexpected keyword argument 'embeddings'
# -> Change line 'embeddings=embeddings,' into 'embedding=embeddings,'
# Result
"""
{'dish_flavor': '이 요리는 ‘비프 스튜’로, 부드럽게 익은 소고기와 감자, 당근이 진한 토마토 베이스의 국물과 어우러져 깊고 풍부한 감칠맛과 은은한 허브 향이 조화를 이루는 따뜻한 풍미가 예상됩니다.',
 'wine_reviews': ": 103099\ncountry: US\ndescription: A smoky nose carries a strong suggestion of bacon fat, wrapped around tightly wound blackberry fruit. Flavors play out broadly and thin out quickly, but this young wine offers solid value and immediate pleasure.\ndesignation: \npoints: 87\nprice: 18.0\nprovince: Oregon\nregion_1: Oregon\nregion_2: Oregon Other\ntaster_name: Paul Gregutt\ntaster_twitter_handle: @paulgwine\ntitle: Jezebel 2013 Pinot Noir (Oregon)\nvariety: Pinot Noir\nwinery: Jezebel\n: 10027\ncountry: US\ndescription: With an inviting strawberry sundae nose, this bright pink rosé of Syrah from the longtime organically minded team at Buttonwood is all about summertime hedonism. There's creaminess on the palate, with some apple tartness in the middle and, again, long strawberry elements on the finish. Pair with beef sliders on soft, sweet rolls that are adorned with gourmet condiments and cheeses.\ndesignation: Estate\npoints: 89\nprice: 18.0\nprovince: California\nregion_1: Santa Ynez Valley\nregion_2: Central Coast\ntaster_name: Matt Kettmann\ntaster_twitter_handle: @mattkettmann\ntitle: Buttonwood 2013 Estate Rosé (Santa Ynez Valley)\nvariety: Rosé\nwinery: Buttonwood\n: 11324\ncountry: US\ndescription: Tarry, savory notes of dried beef, soy, charred lamb and underlying blackberry combine for a nose that screams umami. The palate carries a similar effect of grilled, lavender-covered lamb shank, black peppercorns and black sesame.\ndesignation: \npoints: 93\nprice: 35.0\nprovince: California\nregion_1: Santa Ynez Valley\nregion_2: Central Coast\ntaster_name: Matt Kettmann\ntaster_twitter_handle: @mattkettmann\ntitle: Pace 2013 Syrah (Santa Ynez Valley)\nvariety: Syrah\nwinery: Pace\n: 126414\ncountry: Israel\ndescription: Made from 80% Cabernet Sauvignon and 20% Petit Verdot, this is a ripe and lush wine that boasts big, jammy black-fruit aromas and flavors. Touches of licorice and sage add depth to the palate, while a soft toasty accent lingers on the medium-length finish.\ndesignation: Yogev\npoints: 86\nprice: 15.0\nprovince: Galilee\nregion_1: \nregion_2: \ntaster_name: Lauren Buzzeo\ntaster_twitter_handle: @laurbuzz\ntitle: Binyamina 2010 Yogev Red (Galilee)\nvariety: Bordeaux-style Red Blend\nwinery: Binyamina\n: 41690\ncountry: US\ndescription: With an inviting strawberry sundae nose, this bright pink rosé of Syrah from the longtime organically minded team at Buttonwood is all about summertime hedonism. There's creaminess on the palate, with some apple tartness in the middle and, again, long strawberry elements on the finish. Pair with beef sliders on soft, sweet rolls that are adorned with gourmet condiments and cheeses.\ndesignation: Estate\npoints: 89\nprice: 18.0\nprovince: California\nregion_1: Santa Ynez Valley\nregion_2: Central Coast\ntaster_name: Matt Kettmann\ntaster_twitter_handle: @mattkettmann\ntitle: Buttonwood 2013 Estate Rosé (Santa Ynez Valley)\nvariety: Rosé\nwinery: Buttonwood"}
"""


# 개인 프로젝트: Get your bonds
1. 이슈 템플릿 등록
https://amaran-th.github.io/Github/[Github]%20Issue%20&%20PR%20Template%20%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/
# 기본 제시 템플릿 사용, 추가로 Add feature라는 이름으로 Blank Template 생성
