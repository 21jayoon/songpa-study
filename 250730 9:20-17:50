# LLM : Large Language Model
# 대규모 데이터를 기반으로 학습된 초거대 '언어 모델'로,
# 자연어 생성, 번역, 요약 등 다양한 '언어'관련 작업을 수행할 수 있는 딥러닝 모델이다.

# 가장 근본적인 작업은 "다음에 올 단어 예측하기"

# 트랜스포머 아키텍처, 어텐션 매커니즘 : Transformer - Attention Mechanism
# 인간이 학습하는 과정에서도 모든 단어를 기억하지 않고(모든 것을 다 동일한 비중으로 기억하는 게 아니라)
# 형광펜으로 주요 단어를 밑줄치며 기억하는 등의 과정을 거칠 것임.
# Attention Mechanism은 그런 형광펜의 역할을 하는 매커니즘임. (파라미터가 너무 많기 때문에 점수를 부여해서 주요 단어 예측)

# 환각 Hallucination : 사실이 아닌 정보를 생성하는 것 - 학습 데이터의 불완전성, 과도한 일반화로 인해 발생
# -> 2차 검증이 필요하다

# 구글에 openai api 검색, 우측 상단 API 플랫폼 클릭, 로그인
# API 플랫폼 들어가서 우측 상단 Start building 누르고 Organization 만들 (I'll invite my team later 선택)
# Token 답변에 따라 비용이 측정됨. -> Max tokens를 적당히 작게 잡아야 비용이 덜 나감
# System message에 페르소나(e.g. '당신은 모든 답변을 해적처럼 말하는 유쾌한 AI 선장입니다.')
# User message에는 (아마도) User의 페르소나 설정 가능
# ChatGPT의 temperature(창의성?)은 0부터 2까지임. : 0으로 설정하면 최대한 방어적(보수적)으로 답변함
# 창의성이 낮을수록 예측가능한 일관적인, 사실 위주의 LLM : Large Language Model
# 대규모 데이터를 기반으로 학습된 초거대 '언어 모델'로,
# 자연어 생성, 번역, 요약 등 다양한 '언어'관련 작업을 수행할 수 있는 딥러닝 모델이다.

# 가장 근본적인 작업은 "다음에 올 단어 예측하기"

# 트랜스포머 아키텍처, 어텐션 매커니즘 : Transformer - Attention Mechanism
# 인간이 학습하는 과정에서도 모든 단어를 기억하지 않고(모든 것을 다 동일한 비중으로 기억하는 게 아니라)
# 형광펜으로 주요 단어를 밑줄치며 기억하는 등의 과정을 거칠 것임.
# Attention Mechanism은 그런 형광펜의 역할을 하는 매커니즘임. (파라미터가 너무 많기 때문에 점수를 부여해서 주요 단어 예측)

# 환각 Hallucination : 사실이 아닌 정보를 생성하는 것 - 학습 데이터의 불완전성, 과도한 일반화로 인해 발생
# -> 2차 검증이 필요하다

# 구글에 openai api 검색, 우측 상단 API 플랫폼 클릭, 로그인
# API 플랫폼 들어가서 우측 상단 Start building 누르고 Organization 만들 (I'll invite my team later 선택)
# Token 답변에 따라 비용이 측정됨. -> Max tokens를 적당히 작게 잡아야 비용이 덜 나감
# System message에 페르소나(e.g. '당신은 모든 답변을 해적처럼 말하는 유쾌한 AI 선장입니다.')
# User message에는 (아마도) User의 페르소나 설정 가능
# ChatGPT의 temperature(창의성?)은 0부터 2까지임. : 0으로 설정하면 최대한 방어적(보수적)으로 답변함
# 창의성이 낮을수록 예측가능한 일관적인, 사실 위주의 답변을 한다
